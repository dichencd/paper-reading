# Large Language Models as General Pattern Machines
## Key Points
- in-context learning: LLMs may exhibit more general capabilities of representing and extrapolating symbolic patterns, invariant to the specific tokens involved
- in context learning VS in weights learning [2]
- pattern maching in terms of:
  - sequence transformation
    - token mappig invariance
  - sequence completion
  - sequence improvement
and how these abilities can be applied to robotics at a LOW level (control). NOT high level plans

## limitations
- 

## References
[1] https://arxiv.org/pdf/2307.04721.pdf
[2] https://arxiv.org/pdf/2210.05675.pdf
[3] https://proceedings.neurips.cc/paper_files/paper/2022/file/77c6ccacfd9962e2307fc64680fc5ace-Paper-Conference.pdf