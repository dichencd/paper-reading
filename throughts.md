is language a good mid level representation in autonomous driving?
more efficient representation compared to images, but seems artificial
can we find a representation from images? JPGF??

https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/

difference between robotics tasks (emboided agents, need general understanding) and autonomous driving (specialized)

can internet data help autonomous driving? scene understanding/ decision making??

current robotic tasks are more about understanding an (norval) object, understanding the instruction, which should benefit from internet scale data

you don't need to predict every single detail of the world 
https://www.youtube.com/watch?v=mViTAXCg1xQ&ab_channel=InstituteforExperientialAI


double descent phenomenon
https://openai.com/research/deep-double-descent

Yann Lecun's system 1 and 2. system 1 purely reactive. experienced driver. don't need to think. but it is because it has a very strong internal model --> well trained world model?? inexperienced driver needs to think more
should I do lane change now? but is this over engineering??

in planning, should we make the process more like human driver or more like algorithm?

human does not learn driving by simply watching and imitating


system 1 & system 2 also echoed in Andrej Karparthy's talk

-------
maybe language is not the best form. vision is --> Yann Lecun's framework?