# Direct Preference Optimization: Your Language Model is Secretly a Reward Model
## Abstract
Summary of the paper's abstract.

## Key Points
- need to select the model's desired responses and behavior from its very wide knowledge and abilities
- DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, without an explicit reward function or RL
- ...

## Personal Insights
- Insight 1
- Insight 2
- ...

## Questions/Thoughts
- Question 1
- Thought 1
- ...

## References
[1] https://arxiv.org/pdf/2305.18290.pdf
